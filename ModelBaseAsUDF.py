# Databricks í™˜ê²½ì—ì„œ GLiNER ê¸°ë°˜ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ UDF
# í•„ìš” íŒ¨í‚¤ì§€: pip install gliner pyspark pandas

from pyspark.sql import SparkSession
from pyspark.sql.functions import pandas_udf, col
from pyspark.sql.types import StringType
import pandas as pd
import re

# Spark ì„¸ì…˜ ìƒì„±
spark = SparkSession.builder \
    .appName("GLiNER_PII_Masking") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

# ë¶€ë¶„ ë§ˆìŠ¤í‚¹ í•¨ìˆ˜ë“¤
def mask_name(name):
    """ì´ë¦„ ë¶€ë¶„ ë§ˆìŠ¤í‚¹: í™ê¸¸ë™ â†’ í™*ë™"""
    if len(name) <= 1:
        return name
    elif len(name) == 2:
        return name[0] + '*'
    else:
        return name[0] + '*' * (len(name) - 2) + name[-1]

def mask_phone(phone):
    """ì „í™”ë²ˆí˜¸ ë¶€ë¶„ ë§ˆìŠ¤í‚¹"""
    if '-' in phone:
        parts = phone.split('-')
        if len(parts) == 3:
            return f"{parts[0]}-{parts[1]}-****"
    return phone[:7] + '****' if len(phone) > 7 else phone

def mask_resident_number(rrn):
    """ì£¼ë¯¼ë²ˆí˜¸ ë¶€ë¶„ ë§ˆìŠ¤í‚¹"""
    if '-' in rrn:
        front, back = rrn.split('-')
        return front + '-' + back[0] + '*' * (len(back) - 1)
    elif len(rrn) == 13:
        return rrn[:7] + '*' * 6
    return rrn

def partial_mask_entity(text, entity_type):
    """ì—”í‹°í‹° íƒ€ì…ì— ë”°ë¥¸ ë¶€ë¶„ ë§ˆìŠ¤í‚¹"""
    if entity_type in ["PERSON", "PER"]:
        clean_text = re.sub(r'[ì€ëŠ”ì´ê°€ì„ë¥¼ì—ê²Œê»˜ì„œë‹˜ì”¨êµ°ì–‘]$', '', text)
        suffix = text[len(clean_text):]
        return mask_name(clean_text) + suffix
    elif entity_type in ["PHONE"]:
        return mask_phone(text)
    elif entity_type in ["ID", "RESIDENT_NUMBER"]:
        return mask_resident_number(text)
    elif entity_type in ["ORGANIZATION", "ORG"]:
        return mask_name(text)
    elif entity_type in ["LOCATION", "LOC"]:
        return mask_name(text)
    else:
        return mask_name(text)

# ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ ë¡œë”© (ì„±ëŠ¥ ìµœì í™”)
class GLiNERModelSingleton:
    _instance = None
    _model = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(GLiNERModelSingleton, cls).__new__(cls)
        return cls._instance
    
    def get_model(self):
        if self._model is None:
            try:
                from gliner import GLiNER
                print("GLiNER ëª¨ë¸ ë¡œë”© ì¤‘...")
                self._model = GLiNER.from_pretrained("taeminlee/gliner_ko")
                print("GLiNER ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            except Exception as e:
                print(f"GLiNER ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self._model = None
        return self._model

# ì •ê·œì‹ ê¸°ë°˜ ë°±ì—… ë§ˆìŠ¤í‚¹ (GLiNER ì‹¤íŒ¨ ì‹œ)
def regex_based_masking(text):
    """ì •ê·œì‹ ê¸°ë°˜ ë°±ì—… ë§ˆìŠ¤í‚¹"""
    masked_text = text
    
    # ì£¼ë¯¼ë²ˆí˜¸ íŒ¨í„´ (í•œê¸€ ìˆ«ì í¬í•¨)
    rrn_patterns = [
        r'[ê°€-í£]*[ì˜ì¼ì´ì‚¼ì‚¬ì˜¤ìœ¡ì¹ íŒ”êµ¬ê³µí•˜ë‚˜ë‘˜ì…‹ë„·ë‹¤ì„¯ì—¬ì„¯ì¼ê³±ì—¬ëŸì•„í™‰]{13,}',
        r'\d{6}-\d{7}',
        r'\d{13}'
    ]
    
    for pattern in rrn_patterns:
        matches = list(re.finditer(pattern, masked_text))
        for match in reversed(matches):  # ë’¤ì—ì„œë¶€í„° ì²˜ë¦¬
            original = match.group()
            if len(original) >= 10:  # ì£¼ë¯¼ë²ˆí˜¸ë¡œ ì¶”ì •ë˜ëŠ” ê¸¸ì´
                masked = original[:6] + '*' * (len(original) - 6)
                masked_text = masked_text[:match.start()] + masked + masked_text[match.end():]
    
    # í•œêµ­ ì´ë¦„ íŒ¨í„´
    name_pattern = r'[ê°€-í£]{2,4}(?=ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—ê²Œ|ê»˜ì„œ|ë‹˜|ì”¨)'
    matches = list(re.finditer(name_pattern, masked_text))
    for match in reversed(matches):
        original = match.group()
        masked = mask_name(original)
        masked_text = masked_text[:match.start()] + masked + masked_text[match.end():]
    
    return masked_text

# ë°©ë²• 1: ì¼ë°˜ UDF (ê¶Œì¥)
from pyspark.sql.functions import udf

@udf(StringType())
def gliner_mask_udf(text):
    """GLiNER ê¸°ë°˜ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ UDF (ì¼ë°˜ UDF)"""
    
    if not text or len(text.strip()) == 0:
        return text
    
    # ëª¨ë¸ ë¡œë”© (ì‹±ê¸€í†¤ íŒ¨í„´)
    model_singleton = GLiNERModelSingleton()
    model = model_singleton.get_model()
    
    try:
        if model is not None:
            # GLiNER ëª¨ë¸ ì‚¬ìš©
            labels = ["PERSON", "LOCATION", "ORGANIZATION", "DATE", "PHONE", "ID"]
            entities = model.predict_entities(text, labels)
            
            # ì‹ ë¢°ë„ 0.8 ì´ìƒì¸ ì—”í‹°í‹°ë§Œ ë§ˆìŠ¤í‚¹
            sensitive_entities = [e for e in entities if e.get("score", 0) >= 0.8]
            
            # ìœ„ì¹˜ë³„ ì—­ìˆœ ì •ë ¬
            sensitive_entities = sorted(sensitive_entities, key=lambda e: e['start'], reverse=True)
            
            masked_text = text
            for entity in sensitive_entities:
                original_text = entity['text']
                entity_type = entity['label']
                masked_entity = partial_mask_entity(original_text, entity_type)
                masked_text = masked_text[:entity['start']] + masked_entity + masked_text[entity['end']:]
            
            return masked_text
        else:
            # GLiNER ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ ë°±ì—…
            return regex_based_masking(text)
            
    except Exception as e:
        print(f"ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ì •ê·œì‹ ë°±ì—…
        return regex_based_masking(text)

# ë°©ë²• 2: pandas_udf (ëŒ€ìš©ëŸ‰ ë°ì´í„°ìš©)
@pandas_udf(StringType())
def gliner_mask_pandas_udf(texts: pd.Series) -> pd.Series:
    """GLiNER ê¸°ë°˜ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ pandas_udf (ëŒ€ìš©ëŸ‰ ë°ì´í„°ìš©)"""
    
    # ë°°ì¹˜ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ëª¨ë¸ ë¡œë”©
    model_singleton = GLiNERModelSingleton()
    model = model_singleton.get_model()
    
    def mask_single_text(text):
        if not text or len(text.strip()) == 0:
            return text
            
        try:
            if model is not None:
                labels = ["PERSON", "LOCATION", "ORGANIZATION", "DATE", "PHONE", "ID"]
                entities = model.predict_entities(text, labels)
                
                sensitive_entities = [e for e in entities if e.get("score", 0) >= 0.8]
                sensitive_entities = sorted(sensitive_entities, key=lambda e: e['start'], reverse=True)
                
                masked_text = text
                for entity in sensitive_entities:
                    original_text = entity['text']
                    entity_type = entity['label']
                    masked_entity = partial_mask_entity(original_text, entity_type)
                    masked_text = masked_text[:entity['start']] + masked_entity + masked_text[entity['end']:]
                
                return masked_text
            else:
                return regex_based_masking(text)
                
        except Exception as e:
            return regex_based_masking(text)
    
    return texts.apply(mask_single_text)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°
data = [
    (1, "ìƒë‹´ì‚¬", "ë‹¤ìŒ ë™ì˜ëŠ” ê°€ì… ì„¤ê³„ ë° ë§ì¶¤í˜• ë³´í—˜ ìƒë‹´ ê¸°ì¡´ ê³„ì•½ê³¼ ë¹„êµ ì•ˆë‚´ë¥¼ ìœ„í•œ ì„ íƒ ì‚¬í•­ìœ¼ë¡œ ê±°ë¶€ ê°€ëŠ¥í•˜ë©° ê±°ë¶€ ì‹œ ìì„¸í•œ ìƒë‹´ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
    (2, "ê³ ê°", "ë„¤ ì£¼ë¯¼ë²ˆí˜¸ëŠ” íŒ”ì˜¤í•˜ë‚˜ë‘˜ì¼êµ¬ ë‘˜ë‘˜ì‚¼ì‚¼í•˜ë‚˜í•˜ë‚˜ìœ¡ ì´ì—ìš”"),
    (3, "ìƒë‹´ì‚¬", "ì•„ ë„µ.. íŒ”ì˜¤í•˜ë‚˜ë‘˜ì¼êµ¬ ë‘˜ë‘˜ì‚¼ì‚¼í•˜ë‚˜í•˜ë‚˜ìœ¡"),
    (4, "ê³ ê°", "í™ê¸¸ë™ì´ê³  ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì…ë‹ˆë‹¤."),
    (5, "ìƒë‹´ì‚¬", "ê¹€ì² ìˆ˜ë‹˜ ì•ˆë…•í•˜ì„¸ìš”. ì„œìš¸ ê°•ë‚¨êµ¬ì— ê±°ì£¼í•˜ì‹œëŠ”êµ°ìš”.")
]

# DataFrame ìƒì„±
df = spark.createDataFrame(data, ["line_id", "speaker", "text"])

# ë§ˆìŠ¤í‚¹ ì ìš© - ë‘ ê°€ì§€ ë°©ë²• ì¤‘ ì„ íƒ

# ë°©ë²• 1: ì¼ë°˜ UDF ì‚¬ìš© (ê¶Œì¥ - ì•ˆì •ì„± ìš°ì„ )
df_masked = df.withColumn("masked_text", gliner_mask_udf(col("text")))

# ë°©ë²• 2: pandas_udf ì‚¬ìš© (ëŒ€ìš©ëŸ‰ ë°ì´í„° ì‹œ)
# df_masked = df.withColumn("masked_text", gliner_mask_pandas_udf(col("text")))

# íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ì¶”ê°€
def process_text_files_from_dbfs(input_path, output_path, file_format="text"):
    """DBFSì—ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ë§ˆìŠ¤í‚¹ í›„ ì €ì¥"""
    
    print(f"ğŸ“‚ DBFS íŒŒì¼ ì½ê¸°: {input_path}")
    
    try:
        if file_format == "text":
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° (ê° ì¤„ì´ í•˜ë‚˜ì˜ ë ˆì½”ë“œ)
            df = spark.read.text(input_path)
            df = df.withColumn("line_id", monotonically_increasing_id())
            df = df.select("line_id", col("value").alias("text"))
            
        elif file_format == "csv":
            # CSV íŒŒì¼ ì½ê¸°
            df = spark.read.option("header", "true").csv(input_path)
            df = df.withColumn("line_id", monotonically_increasing_id())
            
        elif file_format == "json":
            # JSON íŒŒì¼ ì½ê¸°
            df = spark.read.json(input_path)
            df = df.withColumn("line_id", monotonically_increasing_id())
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_format}")
        
        print(f"ğŸ“Š ì´ {df.count()}ê°œ ë ˆì½”ë“œ ë¡œë“œë¨")
        
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        text_columns = [col_name for col_name in df.columns if 'text' in col_name.lower() or 'content' in col_name.lower()]
        if not text_columns:
            text_columns = [col_name for col_name in df.columns if col_name not in ['line_id']]
        
        if not text_columns:
            raise ValueError("í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        text_column = text_columns[0]  # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì‚¬ìš©
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì‚¬ìš©: {text_column}")
        
        # ë§ˆìŠ¤í‚¹ ì ìš©
        print("ğŸ”’ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ ì‹œì‘...")
        df_masked = df.withColumn("masked_text", gliner_mask_udf(col(text_column)))
        
        # ê²°ê³¼ ì €ì¥
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        
        if file_format == "text":
            # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥ (ë§ˆìŠ¤í‚¹ëœ í…ìŠ¤íŠ¸ë§Œ)
            df_masked.select("masked_text").write.mode("overwrite").text(output_path)
            
        elif file_format == "csv":
            # CSV íŒŒì¼ë¡œ ì €ì¥ (ì›ë³¸ + ë§ˆìŠ¤í‚¹ ê²°ê³¼)
            df_masked.write.mode("overwrite").option("header", "true").csv(output_path)
            
        elif file_format == "json":
            # JSON íŒŒì¼ë¡œ ì €ì¥
            df_masked.write.mode("overwrite").json(output_path)
        
        # í†µê³„ ì •ë³´
        total_records = df_masked.count()
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {total_records}ê°œ ë ˆì½”ë“œ")
        
        # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“„ ë§ˆìŠ¤í‚¹ ê²°ê³¼ ìƒ˜í”Œ:")
        df_masked.select(text_column, "masked_text").limit(3).show(truncate=False)
        
        return df_masked
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

def process_multiple_file_directories(base_input_path, base_output_path, directories, file_format="text"):
    """ì—¬ëŸ¬ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤ì„ ë°°ì¹˜ ì²˜ë¦¬"""
    
    results = {}
    
    for directory in directories:
        input_path = f"{base_input_path}/{directory}"
        output_path = f"{base_output_path}/{directory}_masked"
        
        print(f"\nğŸ“ ë””ë ‰í† ë¦¬ ì²˜ë¦¬: {directory}")
        
        try:
            df_result = process_text_files_from_dbfs(input_path, output_path, file_format)
            results[directory] = {
                'status': 'success',
                'record_count': df_result.count(),
                'output_path': output_path
            }
        except Exception as e:
            print(f"âŒ {directory} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            results[directory] = {
                'status': 'failed',
                'error': str(e)
            }
    
    # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
    print("\nğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:")
    print("=" * 50)
    
    success_count = 0
    total_records = 0
    
    for directory, result in results.items():
        if result['status'] == 'success':
            success_count += 1
            total_records += result['record_count']
            print(f"âœ… {directory}: {result['record_count']}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ")
        else:
            print(f"âŒ {directory}: ì²˜ë¦¬ ì‹¤íŒ¨ - {result['error']}")
    
    print(f"\nğŸ“Š ì „ì²´ ê²°ê³¼:")
    print(f"ì„±ê³µ: {success_count}/{len(directories)} ë””ë ‰í† ë¦¬")
    print(f"ì´ ì²˜ë¦¬ ë ˆì½”ë“œ: {total_records:,}ê°œ")
    
    return results

def main_databricks_processing(srcDir=None, tgtDir=None, file_format="text", directories=None):
    """ë©”ì¸ Databricks íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜"""
    
    print("ğŸ”’ Databricks GLiNER íŒŒì¼ ë§ˆìŠ¤í‚¹ ë„êµ¬")
    print("=" * 60)
    
    # ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì„¤ì •
    if srcDir is None:
        srcDir = "/dbfs/FileStore/shared_uploads/input/"
        print(f"ğŸ“‚ ê¸°ë³¸ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {srcDir}")
    
    if tgtDir is None:
        tgtDir = "/dbfs/FileStore/shared_uploads/output/"
        print(f"ğŸ“ ê¸°ë³¸ íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {tgtDir}")
    
    print(f"ğŸ“‚ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬: {srcDir}")
    print(f"ğŸ“ íƒ€ê²Ÿ ë””ë ‰í† ë¦¬: {tgtDir}")
    print(f"ğŸ“„ íŒŒì¼ í˜•ì‹: {file_format}")
    
    # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
    try:
        dbutils.fs.ls(srcDir)
        print("âœ… ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸")
    except:
        print(f"âš ï¸ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {srcDir}")
        print("ğŸ§ª ë°ëª¨ìš© ë°ì´í„° ìƒì„±...")
        
        # ë°ëª¨ìš© ë°ì´í„° ìƒì„±
        demo_data = [
            (1, "í™ê¸¸ë™ì€ ì„œìš¸ ê°•ë‚¨êµ¬ì—ì„œ ì‚¬ëŠ”ë°, ì‚¼ì„±ë³‘ì›ì— ì…ì›í–ˆê³ , 2025ë…„ 6ì›”ì— 1ë‹¬ê°„ ë³‘ì›ì— ìˆì—ˆì–´ìš”."),
            (2, "ê¹€ì² ìˆ˜(010-1234-5678)ëŠ” ë¶€ì‚°ëŒ€í•™êµë³‘ì›ì—ì„œ ê³ í˜ˆì•• ì§„ë£Œë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤."),
            (3, "ì´ì˜í¬ë‹˜ì˜ ì£¼ë¯¼ë²ˆí˜¸ëŠ” 123456-1234567ì´ê³ , ì´ë©”ì¼ì€ test@example.comì…ë‹ˆë‹¤."),
            (4, "ë°•ë¯¼ìˆ˜ í™˜ìê°€ í˜„ëŒ€ìë™ì°¨ì— ë‹¤ë‹ˆë©°, ì„œìš¸ì‹œ ì¢…ë¡œêµ¬ ì„¸ì¢…ëŒ€ë¡œ 123ë²ˆì§€ì— ì‚´ê³  ìˆìŠµë‹ˆë‹¤."),
            (5, "ì •ìˆ˜ì§„(35ì„¸)ì€ ë‹¹ë‡¨ë³‘ìœ¼ë¡œ ì¸ìŠë¦°ì„ íˆ¬ì•½ ì¤‘ì´ë©°, ì—°ë´‰ì€ 5000ë§Œì›ì…ë‹ˆë‹¤.")
        ]
        
        # DataFrame ìƒì„± ë° ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        df_demo = spark.createDataFrame(demo_data, ["line_id", "text"])
        
        # ë°ëª¨ íŒŒì¼ ì €ì¥
        demo_srcDir = srcDir + "demo/"
        df_demo.write.mode("overwrite").option("header", "true").csv(demo_srcDir)
        print(f"âœ… ë°ëª¨ ë°ì´í„° ìƒì„±ë¨: {demo_srcDir}")
        
        # ë°ëª¨ ë°ì´í„°ë¡œ ì²˜ë¦¬
        srcDir = demo_srcDir
    
    # ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
    if directories is not None:
        # ì—¬ëŸ¬ ë””ë ‰í† ë¦¬ ë°°ì¹˜ ì²˜ë¦¬
        print(f"ğŸ“ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ: {len(directories)}ê°œ ë””ë ‰í† ë¦¬")
        results = process_multiple_file_directories(srcDir, tgtDir, directories, file_format)
        
        return {
            'mode': 'batch',
            'srcDir': srcDir,
            'tgtDir': tgtDir,
            'directories': directories,
            'results': results,
            'file_format': file_format
        }
    
    else:
        # ë‹¨ì¼ ë””ë ‰í† ë¦¬ ì²˜ë¦¬
        print("ğŸ“„ ë‹¨ì¼ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ëª¨ë“œ")
        df_result = process_text_files_from_dbfs(srcDir, tgtDir, file_format)
        
        return {
            'mode': 'single',
            'srcDir': srcDir,
            'tgtDir': tgtDir,
            'record_count': df_result.count(),
            'file_format': file_format,
            'dataframe': df_result
        }

def setup_databricks_directories(base_path="/dbfs/FileStore/shared_uploads/"):
    """Databricks ë””ë ‰í† ë¦¬ ì„¤ì • ë„ìš°ë¯¸ í•¨ìˆ˜"""
    
    print("ğŸ”§ Databricks ë””ë ‰í† ë¦¬ ì„¤ì •")
    print("=" * 40)
    
    # ê¸°ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
    directories = {
        'input': base_path + "pii_masking/input/",
        'output': base_path + "pii_masking/output/",
        'archive': base_path + "pii_masking/archive/",
        'logs': base_path + "pii_masking/logs/"
    }
    
    for name, path in directories.items():
        try:
            dbutils.fs.mkdirs(path)
            print(f"âœ… {name} ë””ë ‰í† ë¦¬: {path}")
        except Exception as e:
            print(f"âš ï¸ {name} ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
    
    print("\nğŸ“ ì‚¬ìš© ì˜ˆì‹œ:")
    print(f"srcDir = '{directories['input']}'")
    print(f"tgtDir = '{directories['output']}'")
    
    return directories

# ìœ„ì ¯ì„ í†µí•œ ë§¤ê°œë³€ìˆ˜ ì…ë ¥ (Databricks ë…¸íŠ¸ë¶ìš©)
def create_databricks_widgets():
    """Databricks ë…¸íŠ¸ë¶ ìœ„ì ¯ ìƒì„±"""
    
    try:
        # ìœ„ì ¯ ìƒì„±
        dbutils.widgets.text("srcDir", "/dbfs/FileStore/shared_uploads/pii_masking/input/", "ì†ŒìŠ¤ ë””ë ‰í† ë¦¬")
        dbutils.widgets.text("tgtDir", "/dbfs/FileStore/shared_uploads/pii_masking/output/", "íƒ€ê²Ÿ ë””ë ‰í† ë¦¬")
        dbutils.widgets.dropdown("file_format", "text", ["text", "csv", "json"], "íŒŒì¼ í˜•ì‹")
        dbutils.widgets.text("directories", "", "ë°°ì¹˜ ë””ë ‰í† ë¦¬ (ì‰¼í‘œ êµ¬ë¶„, ì„ íƒì‚¬í•­)")
        
        # ìœ„ì ¯ ê°’ ê°€ì ¸ì˜¤ê¸°
        srcDir = dbutils.widgets.get("srcDir")
        tgtDir = dbutils.widgets.get("tgtDir")
        file_format = dbutils.widgets.get("file_format")
        directories_str = dbutils.widgets.get("directories")
        
        directories = None
        if directories_str.strip():
            directories = [d.strip() for d in directories_str.split(",")]
        
        print("ğŸ›ï¸ ìœ„ì ¯ ë§¤ê°œë³€ìˆ˜:")
        print(f"  srcDir: {srcDir}")
        print(f"  tgtDir: {tgtDir}")
        print(f"  file_format: {file_format}")
        print(f"  directories: {directories}")
        
        return srcDir, tgtDir, file_format, directories
        
    except Exception as e:
        print(f"âš ï¸ ìœ„ì ¯ ìƒì„± ì‹¤íŒ¨ (ë…¸íŠ¸ë¶ í™˜ê²½ì´ ì•„ë‹ ìˆ˜ ìˆìŒ): {e}")
        return None, None, "text", None

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ Databricks GLiNER ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ ì‹œì‘")
    print("=" * 50)
    
    # ì‹¤í–‰ ë°©ë²• ì„ íƒ
    execution_mode = input("ì‹¤í–‰ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš” (1: ìœ„ì ¯, 2: ì§ì ‘ì…ë ¥, 3: ë°ëª¨): ").strip()
    
    if execution_mode == "1":
        # ìœ„ì ¯ ì‚¬ìš©
        print("ğŸ›ï¸ Databricks ìœ„ì ¯ ìƒì„±...")
        srcDir, tgtDir, file_format, directories = create_databricks_widgets()
        
        if srcDir is not None:
            result = main_databricks_processing(srcDir, tgtDir, file_format, directories)
        else:
            print("âŒ ìœ„ì ¯ ìƒì„± ì‹¤íŒ¨, ë°ëª¨ ëª¨ë“œë¡œ ì‹¤í–‰")
            result = main_databricks_processing()
    
    elif execution_mode == "2":
        # ì§ì ‘ ì…ë ¥
        print("ğŸ“ ë§¤ê°œë³€ìˆ˜ ì§ì ‘ ì…ë ¥:")
        srcDir = input("ì†ŒìŠ¤ ë””ë ‰í† ë¦¬: ").strip()
        tgtDir = input("íƒ€ê²Ÿ ë””ë ‰í† ë¦¬: ").strip()
        file_format = input("íŒŒì¼ í˜•ì‹ (text/csv/json): ").strip() or "text"
        directories_str = input("ë°°ì¹˜ ë””ë ‰í† ë¦¬ (ì‰¼í‘œ êµ¬ë¶„, ì„ íƒì‚¬í•­): ").strip()
        
        directories = None
        if directories_str:
            directories = [d.strip() for d in directories_str.split(",")]
        
        result = main_databricks_processing(srcDir, tgtDir, file_format, directories)
    
    else:
        # ë°ëª¨ ëª¨ë“œ
        print("ğŸ§ª ë°ëª¨ ëª¨ë“œ ì‹¤í–‰...")
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        dirs = setup_databricks_directories()
        
        result = main_databricks_processing(
            srcDir=dirs['input'],
            tgtDir=dirs['output'],
            file_format="csv"
        )
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(f"  - ëª¨ë“œ: {result['mode']}")
    print(f"  - ì†ŒìŠ¤: {result['srcDir']}")
    print(f"  - íƒ€ê²Ÿ: {result['tgtDir']}")
    print(f"  - íŒŒì¼ í˜•ì‹: {result['file_format']}")
    
    if result['mode'] == 'single':
        print(f"  - ì²˜ë¦¬ ë ˆì½”ë“œ: {result['record_count']:,}ê°œ")
    else:
        success_count = sum(1 for r in result['results'].values() if r['status'] == 'success')
        total_dirs = len(result['directories'])
        print(f"  - ì„±ê³µ ë””ë ‰í† ë¦¬: {success_count}/{total_dirs}")
    
    print("\nğŸ’¡ ì¶”ê°€ ì‚¬ìš© íŒ:")
    print("1. ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì •: df.repartition(200)")
    print("2. ë©”ëª¨ë¦¬ ìµœì í™”: spark.conf.set('spark.sql.adaptive.enabled', 'true')")
    print("3. ê²°ê³¼ ì €ì¥ í˜•ì‹: .csv(), .json(), .parquet() ë“± ì„ íƒ ê°€ëŠ¥")
    print("4. ë°°ì¹˜ í¬ê¸° ì¡°ì •: spark.conf.set('spark.sql.execution.arrow.maxRecordsPerBatch', '1000')")
    print("5. ìœ„ì ¯ ì œê±°: dbutils.widgets.removeAll()")

# Spark ì„¸ì…˜ ì¢…ë£Œ (í•„ìš”ì‹œ)
# spark.stop()
